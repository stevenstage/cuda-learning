{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ec3505",
   "metadata": {},
   "source": [
    "# Cuda Mode Lecture 1\n",
    "## How to profile CUDA kernels in PyTorch\n",
    "Here gives my self-learning notes on CUDA and Triton. As we all know, coding needs to give feedback directly, so I make my determination to run the code and write some feelings and experiences.\n",
    "### Simple example\n",
    "Here is a matrix square example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd71fe21",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def time_pytorch_function(func, input):\n",
    "    # CUDA IS ASYNC so can't use python time module\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        func(input)\n",
    "\n",
    "    start.record()\n",
    "    func(input)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return start.elapsed_time(end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4ed43e",
   "metadata": {},
   "source": [
    "pay attention to the difference between cuda time and pytorch time, if you'd like to test cuda time, it should be note to redefine\n",
    "\n",
    "then turn to an operation with torch square, and look the visualization:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/stevenstage/cuda-learning/blob/main/image/lecture_001/2.png\" width=\"800px\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "so the torch.profiler.profile() is essential to visualize. It clearly shows the time required for some operations. It can be found that aten::pow takes the longest CPU time, and aten::squre takes the longest GPU time, which stands for power and square, respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e407e29",
   "metadata": {},
   "source": [
    "### torch profiler\n",
    "Next, we will use profiler code to observe GPU operations in a more fine-grained manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9621657c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "\n",
    "    schedule=torch.profiler.schedule(\n",
    "        wait=1,\n",
    "        warmup=1,\n",
    "        active=2,\n",
    "        repeat=1),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3709d6ad",
   "metadata": {},
   "source": [
    "We used the ProfilerActivity to further observe the time and memory usage of each operation, as shown in the figure below. Overall, the first two sections help you visualize the process by getting familiar with the profiler\n",
    "<p align=\"center\">\n",
    "  <img src=\"/Users/ldy/blog/image/12.png\" width=\"800px\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f5b9a2",
   "metadata": {},
   "source": [
    "### Custom cpp extensions\n",
    "This part is also relatively simple. Since cuda programming requires c++ and its corresponding compilation environment, there is already a load_inline library in python. You can compile c++ code according to the following code format. In particular, I removed build_directory='./tmp' from the source code, which means downloading the cpp environment based on your own environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f55934",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "\n",
    "cpp_source = \"\"\"\n",
    "std::string hello_world() {\n",
    "  return \"Hello World!\";\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "my_module = load_inline(\n",
    "    name='my_module',\n",
    "    cpp_sources=[cpp_source],\n",
    "    functions=['hello_world'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(my_module.hello_world())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca79c2f8",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/stevenstage/cuda-learning/blob/main/image/lecture_001/3.png\" width=\"800px\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a337c90a",
   "metadata": {},
   "source": [
    "### Integrate a triton kernel\n",
    "Due to triton's strong ease of use and operability, I will emphasize this part in particular. Triton is a \"CUDA alternative for deep learning researchers\", focusing on the high-performance implementation of individual operators on gpus. In Python, the @triton.jit decorator compiles on the spot and caches PTX after the first call. And through torch.empty_strided(...)\" The obtained CUdeviceptr is directly fed to the Triton kernel.\n",
    "\n",
    "Triton can accelerate the traditional path of \"PyTorch eager → cuBLAS → kernel launch\" because it has brought \"operator fusion + tile/ block-level automatic tuning + lightweight compilation\" down to the JIT stage. This thus eliminates a significant amount of overhead related to Python interpreters, framework scheduling, global synchronization, and memory round trips\n",
    "\n",
    "But unfortunately, when I tried to use the triton code in the course myself, I got the same result as slide: it didn't make a difference from the traditional torch. Similarly, the problem occurred in the parameter \"BLOCK WISE\". So I modified the source code and added the function of automatic integration of BLOCK WISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b846326c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE': 256}, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE': 512}, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE': 1024}, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE': 2048}, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE': 4096}, num_warps=16),\n",
    "        triton.Config({'BLOCK_SIZE': 8192}, num_warps=16),\n",
    "        # You can also try different num_warps for the same BLOCK_SIZE\n",
    "        triton.Config({'BLOCK_SIZE': 1024}, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE': 2048}, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE': 2048}, num_warps=16),\n",
    "    ],\n",
    "    key=['n_cols'],  # Auto-tune based on number of columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbed0706",
   "metadata": {},
   "source": [
    "The other code is basically modeled after the original code, and the performance has been significantly improved:\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/stevenstage/cuda-learning/blob/main/image/lecture_001/square()performance.png\" width=\"800px\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe6afad",
   "metadata": {},
   "source": [
    "### ncu profiler\n",
    "The last part, ncu, is very important, but it requires you to have your own GPU to operate. Therefore, I won't show your achievements separately. Just look at the course and slides"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
